# Docker Compose pour Raspberry Pi 4 - Configuration Standalone
# Tout-en-un : Bot Worker + Dashboard + Redis
# Pas de dépendance externe (Synology MySQL/NFS retiré)
#
# Architecture simplifiée:
# - Pi4 : Bot Worker + Dashboard + Redis + SQLite
# - Freebox Pop : IP résidentielle légitime pour LinkedIn
#
# Utilisation:
#   docker compose -f docker-compose.pi4-standalone.yml up -d
#
# Documentation:
# - Setup: docs/RASPBERRY_PI_DOCKER_SETUP.md
# - Troubleshooting: docs/RASPBERRY_PI_TROUBLESHOOTING.md
# - Verification: ./scripts/verify_rpi_docker.sh

services:
  # ========================================
  # REDIS (Queue pour Bot Worker)
  # ========================================
  redis-bot:
    image: redis:7-alpine
    container_name: linkedin-bot-redis
    volumes:
      - redis-bot-data:/data
    # Optimisations Redis pour Pi 4
    # AOF only (no RDB snapshots) to avoid fork-based BGSAVE warnings
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --no-appendfsync-on-rewrite yes
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --loglevel warning
    # Fix memory overcommit warning
    # NOTE: vm.overcommit_memory must be set on the Raspberry Pi host with:
    #   sudo sysctl vm.overcommit_memory=1
    #   echo "vm.overcommit_memory = 1" | sudo tee -a /etc/sysctl.conf
    # sysctls:
    #   - net.core.somaxconn=511
    #   - vm.overcommit_memory=1
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 300M
        reservations:
          cpus: '0.25'
          memory: 200M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - linkedin-network

  # ========================================
  # REDIS (Cache pour Dashboard)
  # ========================================
  redis-dashboard:
    image: redis:7-alpine
    container_name: linkedin-dashboard-redis
    # Cache only (no persistence) to avoid fork operations and memory overcommit warnings
    command: >
      redis-server
      --maxmemory 64mb
      --maxmemory-policy allkeys-lru
      --save ""
      --appendonly no
      --loglevel warning
    restart: unless-stopped
    volumes:
      - redis-dashboard-data:/data
    deploy:
      resources:
        limits:
          memory: 100M
          cpus: '0.5'
        reservations:
          memory: 50M
          cpus: '0.25'
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - linkedin-network

  # ========================================
  # API (Pont entre Dashboard et Worker)
  # ========================================
  api:
    build:
      context: .
      dockerfile: Dockerfile.multiarch
    container_name: linkedin-bot-api
    command: uvicorn src.api.app:app --host 0.0.0.0 --port 8000
    environment:
      - REDIS_HOST=redis-bot
      - REDIS_PORT=6379
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
      - DATABASE_URL=sqlite:///app/data/linkedin.db
      # Clé API pour sécuriser les appels internes
      - API_KEY=internal_secret_key
      # Désactiver la télémétrie OpenTelemetry
      - ENABLE_TELEMETRY=false
    volumes:
      - ./logs:/app/logs
      - ./config:/app/config
      - ./auth_state.json:/app/auth_state.json:ro
      - ./messages.txt:/app/messages.txt
      - ./late_messages.txt:/app/late_messages.txt
      - shared-data:/app/data
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 600M # Increased to handle Playwright Auth flow
        reservations:
          memory: 300M
    restart: unless-stopped
    networks:
      - linkedin-network

  # ========================================
  # BOT WORKER (RQ Worker avec Playwright)
  # ========================================
  bot-worker:
    build:
      context: .
      dockerfile: Dockerfile.multiarch
    container_name: linkedin-bot-worker
    command: python -m src.queue.worker
    depends_on:
      redis-bot:
        condition: service_healthy
    environment:
      - REDIS_HOST=redis-bot
      - REDIS_PORT=6379
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
      # Utilisation SQLite partagé avec le dashboard
      - DATABASE_URL=sqlite:///app/data/linkedin.db
      # Désactiver la télémétrie OpenTelemetry
      - ENABLE_TELEMETRY=false
    volumes:
      - ./logs:/app/logs
      - ./config:/app/config
      - ./auth_state.json:/app/auth_state.json
      - ./messages.txt:/app/messages.txt
      - ./late_messages.txt:/app/late_messages.txt
      - shared-data:/app/data
    # Optimisé pour Pi 4 avec 4GB RAM
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 900M
        reservations:
          cpus: '0.5'
          memory: 450M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import redis; r = redis.Redis(host='redis-bot', port=6379); r.ping()"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - linkedin-network

  # ========================================
  # DASHBOARD (Next.js avec SQLite)
  # ========================================
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile.prod.pi4
    restart: unless-stopped
    container_name: linkedin-dashboard
    ports:
      - "${DASHBOARD_PORT:-3000}:3000"
    deploy:
      resources:
        limits:
          memory: 400M    # Optimisé après suppression de Chromium
          cpus: '1.0'
        reservations:
          memory: 200M
          cpus: '0.25'
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    environment:
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1

      # ✅ SQLite Local
      - DATABASE_URL=sqlite:///app/data/linkedin.db

      # Configuration Redis Dashboard
      - REDIS_URL=redis://redis-dashboard:6379

      # Configuration Bot (pour Proxy API)
      - BOT_API_URL=http://api:8000
      - BOT_API_KEY=internal_secret_key
    volumes:
      # Logs partagés avec le bot
      - ./logs:/app/logs
      # Fichiers de configuration et messages (RW pour édition depuis Dashboard)
      - ./config:/app/config
      - ./messages.txt:/app/messages.txt
      - ./late_messages.txt:/app/late_messages.txt
      # Base de données SQLite partagée avec le bot
      - shared-data:/app/data
    depends_on:
      redis-dashboard:
        condition: service_healthy
      api:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/system/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - linkedin-network

# ========================================
# VOLUMES PERSISTANTS
# ========================================
volumes:
  redis-bot-data:
    name: linkedin-bot-redis-data
  redis-dashboard-data:
    name: linkedin-dashboard-redis-data
  shared-data:
    name: linkedin-shared-data
    # Volume partagé entre le bot et le dashboard pour SQLite

# ========================================
# RÉSEAU INTERNE
# ========================================
networks:
  linkedin-network:
    name: linkedin-network
    driver: bridge
