# Docker Compose pour Raspberry Pi 4 - Configuration Standalone
# Tout-en-un : Bot Worker + Dashboard + Redis + Nginx (Proxy)
# Pas de d√©pendance externe (Synology MySQL/NFS retir√©)
#
# Architecture simplifi√©e:
# - Pi4 : Bot Worker + Dashboard + Redis + SQLite + Nginx
# - Freebox Pop : IP r√©sidentielle l√©gitime pour LinkedIn
#
# Utilisation:
#   docker compose -f docker-compose.pi4-standalone.yml up -d
#
# Documentation:
# - Setup: docs/RASPBERRY_PI_DOCKER_SETUP.md
# - Troubleshooting: docs/RASPBERRY_PI_TROUBLESHOOTING.md
# - Verification: ./scripts/verify_rpi_docker.sh

services:
  # ========================================
  # REDIS (Queue pour Bot Worker)
  # ========================================
  redis-bot:
    image: redis:7-alpine
    container_name: redis-bot
    volumes:
    - redis-bot-data:/data
    # Optimisations Redis pour Pi 4
    # AOF only (no RDB snapshots) to avoid fork-based BGSAVE warnings
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --no-appendfsync-on-rewrite yes
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --loglevel warning
    # Fix memory overcommit warning
    sysctls:
      - net.core.somaxconn=511
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 300M
        reservations:
          cpus: '0.25'
          memory: 200M
    logging:
      driver: json-file
      options:
        max-size: 5m
        max-file: '2'
        compress: 'true'
    restart: unless-stopped
    healthcheck:
      test: [CMD, redis-cli, ping]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
    - linkedin-network

  # ========================================
  # REDIS (Cache pour Dashboard)
  # ========================================
  redis-dashboard:
    image: redis:7-alpine
    container_name: redis-dashboard
    # Cache only (no persistence) to avoid fork operations and memory overcommit warnings
    command: >
      redis-server
      --maxmemory 64mb
      --maxmemory-policy allkeys-lru
      --save ""
      --appendonly no
      --loglevel warning
    # Fix memory overcommit warning
    sysctls:
      - net.core.somaxconn=511
    restart: unless-stopped
    volumes:
    - redis-dashboard-data:/data
    deploy:
      resources:
        limits:
          memory: 100M
          cpus: '0.5'
        reservations:
          memory: 50M
          cpus: '0.25'
    logging:
      driver: json-file
      options:
        max-size: 5m
        max-file: '2'
        compress: 'true'
    healthcheck:
      test: [CMD, redis-cli, ping]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
    - linkedin-network

  # ========================================
  # API (Pont entre Dashboard et Worker)
  # ========================================
  api:
    image: ghcr.io/gaspardd78/linkedin-birthday-auto-bot:latest
    container_name: bot-api
    ports:
    - "8000:8000"
    command: >
      sh -c "pip install schedule opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation-fastapi grpcio &&
      uvicorn src.api.app:app --host 0.0.0.0 --port 8000"
    # Enable privileged mode to allow systemctl commands
    privileged: true
    environment:
    - REDIS_HOST=redis-bot
    - REDIS_PORT=6379
    - PYTHONPATH=/app
    - LOG_LEVEL=INFO
    - DATABASE_URL=sqlite:///app/data/linkedin.db
      # Cl√© API pour s√©curiser les appels internes (OBLIGATOIRE)
      # G√©n√©rer avec: python -c "import secrets; print(secrets.token_hex(32))"
    - API_KEY=${API_KEY:?API_KEY must be set in .env file}
      # Activer la t√©l√©m√©trie OpenTelemetry
    - ENABLE_TELEMETRY=false
      # Nom du service pour s√©parer les logs
    - SERVICE_NAME=api
      # Force l'utilisation du fichier config mont√© (pas celui baked-in dans l'image)
    - LINKEDIN_BOT_CONFIG_PATH=/app/config/config.yaml
    volumes:
    - ./logs:/app/logs
    - ./config:/app/config
    # Messages files are stored in /app/data/ and editable via API
    # auth_state.json is optional - can be uploaded to /app/data/auth_state.json via dashboard
    - ./data:/app/data
    # Mount systemd and D-Bus to allow systemctl commands from container
    - /run/systemd:/run/systemd:ro
    - /var/run/dbus:/var/run/dbus:ro
    - /bin/systemctl:/bin/systemctl:ro
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 500M
        reservations:
          memory: 200M
    restart: unless-stopped
    healthcheck:
      # CRASH PROOF: Use Python healthcheck instead of curl for robustness
      # as recommended in GUIDE_INSTALLATION_SIMPLIFIEE.md
      test: ["CMD", "python", "-c", "import urllib.request; print(urllib.request.urlopen('http://localhost:8000/health').read())"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    networks:
    - linkedin-network

  # ========================================
  # BOT WORKER (RQ Worker avec Playwright)
  # ========================================
  bot-worker:
    image: ghcr.io/gaspardd78/linkedin-birthday-auto-bot:latest
    container_name: bot-worker
    command: >
      sh -c "pip install schedule opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation-fastapi grpcio &&
      python -m src.queue.worker"
    depends_on:
      redis-bot:
        condition: service_healthy
    environment:
    - REDIS_HOST=redis-bot
    - REDIS_PORT=6379
    - PYTHONPATH=/app
    - LOG_LEVEL=INFO
      # Utilisation SQLite partag√© avec le dashboard
    - DATABASE_URL=sqlite:///app/data/linkedin.db
      # Activer la t√©l√©m√©trie OpenTelemetry
    - ENABLE_TELEMETRY=false
      # Nom du service pour s√©parer les logs
    - SERVICE_NAME=worker
      # Force l'utilisation du fichier config mont√© (pas celui baked-in dans l'image)
    - LINKEDIN_BOT_CONFIG_PATH=/app/config/config.yaml
    volumes:
    - ./logs:/app/logs
    - ./config:/app/config
    # Messages files are stored in /app/data/ and editable via API
    # auth_state.json is optional - can be in environment variable or uploaded to /app/data/
    - ./data:/app/data
    # üöÄ OPTIMIS√â POUR RASPBERRY PI 4 (Multi-process with limits for stability)
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1800M  # Increased to support 1024MB browser heap + Python/Playwright overhead
        reservations:
          cpus: '0.5'
          memory: 512M
    memswap_limit: 2000M  # Increased swap limit to match new memory requirements
    logging:
      driver: json-file
      options:
        max-size: 5m
        max-file: '2'
        compress: 'true'
    restart: unless-stopped
    healthcheck:
      test: [CMD, python, -c, "import redis; r = redis.Redis(host='redis-bot', port=6379); r.ping()"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
    - linkedin-network

  # ========================================
  # DASHBOARD (Next.js avec SQLite)
  # ========================================
  dashboard:
    # Dashboard utilise l'image officielle depuis GHCR
    # (plus de build local pour √©conomiser ressources Pi4)
    image: ghcr.io/gaspardd78/linkedin-birthday-auto-dashboard:latest
    pull_policy: always
    restart: unless-stopped
    container_name: dashboard
    ports:
    - ${DASHBOARD_PORT:-3000}:3000
    deploy:
      resources:
        limits:
          memory: 400M    # Optimis√© apr√®s suppression de Chromium
          cpus: '2.0'
    logging:
      driver: json-file
      options:
        max-size: 5m
        max-file: '2'
        compress: 'true'
    env_file: .env
    environment:
    - NODE_ENV=production
    - NEXT_TELEMETRY_DISABLED=1
    - HOSTNAME=0.0.0.0
      # Optimisation M√©moire pour RPi4 (√©viter OOM Crash)
    - NODE_OPTIONS=--max-old-space-size=256

      # ‚úÖ SQLite Local
    - DATABASE_URL=sqlite:///app/data/linkedin.db

      # Configuration Redis Dashboard
    - REDIS_URL=redis://redis-dashboard:6379

      # Configuration Bot (pour Proxy API)
    - BOT_API_URL=http://api:8000
    - BOT_API_KEY=${API_KEY:?API_KEY must be set in .env file}

      # Configuration Authentification Dashboard (REQUIS)
    - JWT_SECRET=${JWT_SECRET}
    - DASHBOARD_USER=${DASHBOARD_USER}
    - DASHBOARD_PASSWORD=${DASHBOARD_PASSWORD}

      # Nom du service pour s√©parer les logs
    - SERVICE_NAME=dashboard
    volumes:
      # Logs partag√©s avec le bot
    - ./logs:/app/logs
      # Fichiers de configuration et messages (RW pour √©dition depuis Dashboard)
    - ./config:/app/config
      # Base de donn√©es SQLite partag√©e avec le bot
    - ./data:/app/data
      # Monitoring de temp√©rature du Raspberry Pi 4 (lecture seule)
    - /sys/class/thermal:/sys/class/thermal:ro
    depends_on:
      redis-dashboard:
        condition: service_healthy
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/system/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    networks:
    - linkedin-network

  # ========================================
  # REVERSE PROXY (Nginx + SSL)
  # ========================================
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./deployment/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./deployment/nginx/linkedin-bot.conf:/etc/nginx/conf.d/default.conf
      - ./deployment/nginx/rate-limit-zones.conf:/etc/nginx/conf.d/rate-limit-zones.conf
      - ./deployment/nginx/429.html:/var/www/html/429.html
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot
    depends_on:
      - dashboard
      - api
    networks:
      - linkedin-network

  # ========================================
  # MONITORING (Prometheus + Grafana)
  # ========================================
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    deploy:
      resources:
        limits:
          memory: 200M
    restart: unless-stopped
    networks:
      - linkedin-network

  grafana:
    image: grafana/grafana:10.0.3
    container_name: grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    ports:
      - "3001:3000"
    deploy:
      resources:
        limits:
          memory: 150M
    restart: unless-stopped
    networks:
      - linkedin-network

  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: node-exporter
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'
    deploy:
      resources:
        limits:
          memory: 50M
    networks:
      - linkedin-network

# ========================================
# VOLUMES PERSISTANTS
# ========================================
volumes:
  redis-bot-data:
    name: linkedin-bot-redis-data
  redis-dashboard-data:
    name: linkedin-dashboard-redis-data
  prometheus_data:
    name: linkedin-prometheus-data
  grafana_data:
    name: linkedin-grafana-data
  # shared-data removed in favor of ./data bind mount

# ========================================
# R√âSEAU INTERNE
# ========================================
networks:
  linkedin-network:
    name: linkedin-network
    driver: bridge
