# Docker Compose pour Raspberry Pi 4 - Configuration Standalone
# Tout-en-un : Bot Worker + Dashboard + Redis
# Pas de dépendance externe (Synology MySQL/NFS retiré)
#
# Architecture simplifiée:
# - Pi4 : Bot Worker + Dashboard + Redis + SQLite
# - Freebox Pop : IP résidentielle légitime pour LinkedIn
#
# Utilisation:
#   docker compose -f docker-compose.pi4-standalone.yml up -d
#
# Documentation:
# - Setup: docs/RASPBERRY_PI_DOCKER_SETUP.md
# - Troubleshooting: docs/RASPBERRY_PI_TROUBLESHOOTING.md
# - Verification: ./scripts/verify_rpi_docker.sh

services:
  # ========================================
  # REDIS (Queue pour Bot Worker)
  # ========================================
  redis-bot:
    image: redis:7-alpine
    container_name: linkedin-bot-redis
    volumes:
      - redis-bot-data:/data
    # Optimisations Redis pour Pi 4
    command: >
      redis-server
      --appendonly yes
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --loglevel warning
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 300M
        reservations:
          cpus: '0.25'
          memory: 200M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    restart: unless-stopped
    # Fix Redis memory overcommit warning
    sysctls:
      - net.core.somaxconn=511
      - vm.overcommit_memory=1
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - linkedin-network

  # ========================================
  # REDIS (Cache pour Dashboard)
  # ========================================
  redis-dashboard:
    image: redis:7-alpine
    container_name: linkedin-dashboard-redis
    command: >
      redis-server
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --loglevel warning
    restart: unless-stopped
    volumes:
      - redis-dashboard-data:/data
    deploy:
      resources:
        limits:
          memory: 150M
          cpus: '0.5'
        reservations:
          memory: 100M
          cpus: '0.25'
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    # Fix Redis memory overcommit warning
    sysctls:
      - net.core.somaxconn=511
      - vm.overcommit_memory=1
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - linkedin-network

  # ========================================
  # API (Pont entre Dashboard et Worker)
  # ========================================
  api:
    build:
      context: .
      dockerfile: Dockerfile.multiarch
    container_name: linkedin-bot-api
    command: uvicorn src.api.app:app --host 0.0.0.0 --port 8000
    environment:
      - REDIS_HOST=redis-bot
      - REDIS_PORT=6379
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
      - DATABASE_URL=sqlite:///app/data/linkedin.db
      # Clé API pour sécuriser les appels internes
      - API_KEY=internal_secret_key
    volumes:
      - ./logs:/app/logs
      - ./config:/app/config
      - ./auth_state.json:/app/auth_state.json:ro
      - ./messages.txt:/app/messages.txt
      - ./late_messages.txt:/app/late_messages.txt
      - shared-data:/app/data
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 200M
        reservations:
          memory: 100M
    restart: unless-stopped
    networks:
      - linkedin-network

  # ========================================
  # BOT WORKER (RQ Worker avec Playwright)
  # ========================================
  bot-worker:
    build:
      context: .
      dockerfile: Dockerfile.multiarch
    container_name: linkedin-bot-worker
    command: python -m src.queue.worker
    depends_on:
      redis-bot:
        condition: service_healthy
    environment:
      - REDIS_HOST=redis-bot
      - REDIS_PORT=6379
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
      # Utilisation SQLite partagé avec le dashboard
      - DATABASE_URL=sqlite:///app/data/linkedin.db
    volumes:
      - ./logs:/app/logs
      - ./config:/app/config
      - ./auth_state.json:/app/auth_state.json
      - ./messages.txt:/app/messages.txt
      - ./late_messages.txt:/app/late_messages.txt
      - shared-data:/app/data
    # Optimisé pour Pi 4 avec 4GB RAM
    # Bot Worker: 900MB (Chromium + Python) - Optimisé après audit
    # Dashboard: 700MB - Optimisé après audit
    # Redis: 300MB (2x)
    # API: 200MB
    # Système: ~1GB
    # = ~2.4GB utilisés, laisse ~1.6GB libre (marge de sécurité)
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 900M
        reservations:
          cpus: '0.5'
          memory: 450M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import redis; r = redis.Redis(host='redis-bot', port=6379); r.ping()"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - linkedin-network

  # ========================================
  # DASHBOARD (Next.js avec SQLite)
  # ========================================
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile.prod.pi4
    restart: unless-stopped
    container_name: linkedin-dashboard
    ports:
      - "${DASHBOARD_PORT:-3000}:3000"
    deploy:
      resources:
        limits:
          memory: 700M    # Optimisé après audit pour éviter le swap
          cpus: '1.0'
        reservations:
          memory: 350M
          cpus: '0.25'
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        compress: "true"
    environment:
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1

      # ✅ SQLite Local (pas de MySQL Synology)
      - DATABASE_URL=sqlite:///app/data/linkedin.db

      # Configuration Redis Dashboard
      - REDIS_URL=redis://redis-dashboard:6379

      # Configuration Puppeteer (si utilisé dans le dashboard)
      - HEADLESS=true
      - PUPPETEER_ARGS=--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-gpu

      # Configuration Bot (optionnel)
      - BOT_REDIS_URL=redis://redis-bot:6379
      - BOT_REDIS_HOST=redis-bot
      - BOT_REDIS_PORT=6379
      - BOT_API_URL=http://linkedin-bot-api:8000
      - BOT_API_KEY=internal_secret_key
    volumes:
      # Logs partagés avec le bot
      - ./logs:/app/logs
      # Fichiers de configuration et messages (RW pour édition depuis Dashboard)
      - ./config:/app/config
      - ./messages.txt:/app/messages.txt
      - ./late_messages.txt:/app/late_messages.txt
      # Base de données SQLite partagée avec le bot
      - shared-data:/app/data
    depends_on:
      redis-dashboard:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/system/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - linkedin-network

# ========================================
# VOLUMES PERSISTANTS
# ========================================
volumes:
  redis-bot-data:
    name: linkedin-bot-redis-data
  redis-dashboard-data:
    name: linkedin-dashboard-redis-data
  shared-data:
    name: linkedin-shared-data
    # Volume partagé entre le bot et le dashboard pour SQLite

# ========================================
# RÉSEAU INTERNE
# ========================================
networks:
  linkedin-network:
    name: linkedin-network
    driver: bridge
