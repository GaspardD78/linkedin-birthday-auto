# Docker Compose pour Raspberry Pi 4 - Configuration Standalone
# Tout-en-un : Bot Worker + Dashboard + Redis + Nginx (Proxy)
# Pas de dépendance externe (Synology MySQL/NFS retiré)
#
# Architecture simplifiée:
# - Pi4 : Bot Worker + Dashboard + Redis + SQLite + Nginx
# - Freebox Pop : IP résidentielle légitime pour LinkedIn
#
# Utilisation:
#   docker compose -f docker-compose.pi4-standalone.yml up -d
#
# ═══════════════════════════════════════════════════════════════════════════
# AUDIT V3.0 - ZERO-WRITE ARCHITECTURE (SD CARD PROTECTION)
# ═══════════════════════════════════════════════════════════════════════════
#
# [OPTIM-1] TMPFS (RAM Disk) pour tous les fichiers éphémères
#           - Logs (/app/logs)
#           - Playwright Caches (/ms-playwright/chromium-...)
#           - Temp (/tmp)
#           → Élimine 95% des écritures disques inutiles sur la carte SD.
#
# [OPTIM-2] LOGGING DRIVER "LOCAL"
#           - Rotation stricte (10MB max) pour éviter saturation RAM/Disk
#
# [OPTIM-3] LIMITES CPU STRICTES
#           - Pas de limite RAM (Kernel RPi non supporté)
#           - Limites CPU pour éviter surchauffe

services:
  # ========================================
  # REDIS (Queue pour Bot Worker)
  # ========================================
  redis-bot:
    image: redis:7-alpine
    container_name: redis-bot
    # [ZERO-WRITE] Pas de volume persistant pour la Queue Redis
    # En cas de reboot, la queue est perdue, ce qui est acceptable pour des jobs workers
    # On évite l'écriture disque AOF/RDB
    command: >
      redis-server
      --save ""
      --appendonly no
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --loglevel warning
      --tcp-backlog 511
    deploy:
      resources:
        limits:
          cpus: '0.5'
        reservations:
          cpus: '0.25'
    logging: &logging_config
      driver: local
      options:
        max-size: 10m
        max-file: '3'
        compress: 'true'
    restart: unless-stopped
    healthcheck:
      test: [CMD, redis-cli, ping]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
    - linkedin-network

  # ========================================
  # REDIS (Cache pour Dashboard)
  # ========================================
  redis-dashboard:
    image: redis:7-alpine
    container_name: redis-dashboard
    # [ZERO-WRITE] Cache purement en RAM
    command: >
      redis-server
      --save ""
      --appendonly no
      --maxmemory 64mb
      --maxmemory-policy allkeys-lru
      --loglevel warning
      --tcp-backlog 511
    restart: unless-stopped
    # [ZERO-WRITE] Pas de volume disque
    deploy:
      resources:
        limits:
          cpus: '0.5'
        reservations:
          cpus: '0.25'
    logging: *logging_config
    healthcheck:
      test: [CMD, redis-cli, ping]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
    - linkedin-network

  # ========================================
  # API (Pont entre Dashboard et Worker)
  # ========================================
  api:
    image: ghcr.io/gaspardd78/linkedin-birthday-auto-bot:latest
    container_name: bot-api
    ports:
    - "8000:8000"
    depends_on:
      redis-bot:
        condition: service_healthy
    command: >
      sh -c "pip install -r /app/requirements.txt &&
      pip install schedule opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation-fastapi grpcio &&
      uvicorn src.api.app:app --host 0.0.0.0 --port 8000"
    # Privileged requis uniquement si besoin de commandes système, sinon à retirer
    privileged: false
    # Sécurité: Read-Only Root FS (sauf tmpfs)
    read_only: true
    user: "1000:1000"
    environment:
    - REDIS_HOST=redis-bot
    - REDIS_PORT=6379
    - PYTHONPATH=/app
    - LOG_LEVEL=INFO
    - DATABASE_URL=sqlite:///app/data/linkedin.db
    - API_KEY=${API_KEY:?API_KEY must be set in .env file}
    - ENABLE_TELEMETRY=false
    - SERVICE_NAME=api
    - LINKEDIN_BOT_CONFIG_PATH=/app/config/config.yaml
    # [ZERO-WRITE] TMPFS MOUNTS
    # Les logs et fichiers temporaires vont en RAM.
    tmpfs:
      - /app/logs
      - /tmp
      - /var/tmp
      - /root/.cache
    volumes:
    - ./config:/app/config:ro  # Read-Only config
    - ./data:/app/data:rw      # RW Data (DB is here)
    deploy:
      resources:
        limits:
          cpus: '0.5'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; print(urllib.request.urlopen('http://localhost:8000/health').read())"]
      interval: 30s
      timeout: 10s
      retries: 15
      start_period: 180s
    dns:
      - 1.1.1.1
      - 8.8.8.8
    logging: *logging_config
    networks:
    - linkedin-network

  # ========================================
  # BOT WORKER (RQ Worker avec Playwright)
  # ========================================
  bot-worker:
    image: ghcr.io/gaspardd78/linkedin-birthday-auto-bot:latest
    container_name: bot-worker
    depends_on:
      redis-bot:
        condition: service_healthy
    command: >
      sh -c "pip install -r /app/requirements.txt &&
      pip install schedule opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation-fastapi grpcio &&
      python -m src.queue.worker"
    user: "1000:1000"
    # Read-Only Root FS pour sécurité
    read_only: true
    environment:
    - REDIS_HOST=redis-bot
    - REDIS_PORT=6379
    - PYTHONPATH=/app
    - LOG_LEVEL=INFO
    - DATABASE_URL=sqlite:///app/data/linkedin.db
    - ENABLE_TELEMETRY=false
    - SERVICE_NAME=worker
    - LINKEDIN_BOT_CONFIG_PATH=/app/config/config.yaml
    # [ZERO-WRITE] TMPFS MOUNTS CRITIQUES
    # Playwright écrit énormément dans /tmp et cache browser. Tout doit aller en RAM.
    tmpfs:
      - /app/logs
      - /tmp
      - /var/tmp
      - /home/appuser/.cache
      # Important: Playwright browser contexts
      # Note: /ms-playwright est le dossier d'install (Read-Only), mais les profils users
      # sont créés à la volée, souvent dans /tmp ou user home.
    volumes:
    - ./config:/app/config:ro
    - ./data:/app/data:rw
    deploy:
      resources:
        limits:
          cpus: '1.5'
        reservations:
          cpus: '0.5'
    logging: *logging_config
    restart: unless-stopped
    healthcheck:
      test: [CMD, python, -c, "import redis; r = redis.Redis(host='redis-bot', port=6379); r.ping()"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    dns:
      - 1.1.1.1
      - 8.8.8.8
    networks:
    - linkedin-network

  # ========================================
  # DASHBOARD (Next.js avec SQLite)
  # ========================================
  dashboard:
    image: ghcr.io/gaspardd78/linkedin-birthday-auto-dashboard:latest
    pull_policy: missing
    restart: unless-stopped
    container_name: dashboard
    ports:
    - ${DASHBOARD_PORT:-3000}:3000
    deploy:
      resources:
        limits:
          cpus: '2.0'
    logging: *logging_config
    env_file: .env
    environment:
    - NODE_ENV=production
    - NEXT_TELEMETRY_DISABLED=1
    - HOSTNAME=0.0.0.0
    - NODE_OPTIONS=--max-old-space-size=256
    - DATABASE_URL=sqlite:///app/data/linkedin.db
    - REDIS_URL=redis://redis-dashboard:6379
    - BOT_API_URL=http://api:8000
    - BOT_API_KEY=${API_KEY:?API_KEY must be set in .env file}
    - JWT_SECRET=${JWT_SECRET}
    - DASHBOARD_USER=${DASHBOARD_USER}
    - DASHBOARD_PASSWORD=${DASHBOARD_PASSWORD}
    - SERVICE_NAME=dashboard
    # [ZERO-WRITE] Logs en RAM
    tmpfs:
      - /app/logs
      - /tmp
    volumes:
    - ./config:/app/config:rw # RW pour édition depuis UI
    - ./data:/app/data:rw
    # Pas de read_only: true car Next.js a besoin d'écrire dans .next/cache parfois (sauf si configuré autrement)
    # Mais ici on utilise l'image prod optimisée.
    depends_on:
      redis-dashboard:
        condition: service_healthy
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/system/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    networks:
    - linkedin-network

  # ========================================
  # REVERSE PROXY (Nginx + SSL)
  # ========================================
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    # [ZERO-WRITE] Logs Nginx en RAM
    # Rediriger access.log et error.log vers stdout/stderr est le défaut Docker,
    # donc géré par le logging driver (local).
    # Mais si Nginx écrit des fichiers cache/temp :
    tmpfs:
      - /var/cache/nginx
      - /run
      - /tmp
    volumes:
      - ./deployment/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./deployment/nginx/linkedin-bot.conf:/etc/nginx/conf.d/default.conf:ro
      - ./deployment/nginx/rate-limit-zones.conf:/etc/nginx/conf.d/rate-limit-zones.conf:ro
      - ./deployment/nginx/options-ssl-nginx.conf:/etc/nginx/conf.d/options-ssl-nginx.conf:ro
      - ./deployment/nginx/ssl-dhparams.pem:/etc/nginx/conf.d/ssl-dhparams.pem:ro
      - ./deployment/nginx/429.html:/var/www/html/429.html:ro
      - ./certbot/conf:/etc/letsencrypt:ro
      - ./certbot/www:/var/www/certbot:ro
    depends_on:
      - dashboard
      - api
    dns:
      - 1.1.1.1
      - 8.8.8.8
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - linkedin-network

  # ========================================
  # MONITORING (Prometheus + Grafana)
  # ========================================
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      # Volume persistant nécessaire pour l'historique, mais on pourrait le mettre en tmpfs
      # si on accepte de perdre l'historique au reboot.
      # Pour RPi4 SD Saver : On garde le volume mais on configure la rétention courte (15j déjà fait)
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      # [OPTIM] Réduire la fréquence de flush WAL si possible via flags (pas standard sur prom)
    deploy:
      resources:
        limits:
          cpus: '1.0'
    restart: unless-stopped
    networks:
      - linkedin-network

  grafana:
    image: grafana/grafana:10.0.3
    container_name: grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_DATABASE_WAL=true
      # [ZERO-WRITE] Logs en mode console uniquement (pas de fichiers)
      - GF_LOG_MODE=console
    ports:
      - "3001:3000"
    deploy:
      resources:
        limits:
          cpus: '0.5'
    restart: unless-stopped
    dns:
      - 1.1.1.1
      - 8.8.8.8
    networks:
      - linkedin-network

  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: node-exporter
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'
    deploy:
      resources:
        limits:
          cpus: '0.25'
    networks:
      - linkedin-network

volumes:
  # Volumes nommés (stockés dans /var/lib/docker/volumes par défaut)
  # Pour SD Saver, s'assurer que /var/lib/docker n'est pas sur une partition saturée.
  redis-bot-data: # SUPPRIMÉ (Zero-Write) - Redis en RAM
  redis-dashboard-data: # SUPPRIMÉ (Zero-Write)
  prometheus_data:
    name: linkedin-prometheus-data
  grafana_data:
    name: linkedin-grafana-data

networks:
  linkedin-network:
    name: linkedin-network
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_ip_masquerade: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
